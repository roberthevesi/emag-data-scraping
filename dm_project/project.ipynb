{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "%pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_price_for_product_from_html(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    scripts = soup.find_all('script')\n",
    "    for script in scripts:\n",
    "        if 'EM.product_id' in script.text:\n",
    "            product_id_search = re.search(r'EM.productDiscountedPrice\\s=\\s([0-9.]+);', script.text)\n",
    "            if product_id_search:\n",
    "                return float(product_id_search.group(1))\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_reviews_for_product_by_url(product_url):\n",
    "    # remove potential '?' at the end of the url\n",
    "    if product_url.endswith('?'):\n",
    "        product_url = product_url[:-1]\n",
    "\n",
    "    # remove potential '#used-products' at the end of the url\n",
    "    if product_url.endswith('#used-products'):\n",
    "        product_url = product_url[:-len('#used-products')]\n",
    "\n",
    "    # check for potentially missing slash at the very end\n",
    "    if not product_url.endswith('/'):\n",
    "        product_url += '/'\n",
    "\n",
    "    # --------------------- endpoint URL ---------------------\n",
    "\n",
    "    substr_to_remove = 'https://www.emag.ro/'\n",
    "    endpoint_url = 'https://www.emag.ro/product-feedback/'\n",
    "    endpoint_url += product_url.replace(substr_to_remove, '')\n",
    "    endpoint_url += 'reviews/list'\n",
    "\n",
    "    # --------------------- GET request ---------------------\n",
    "\n",
    "    product_response = requests.get(product_url)\n",
    "    product_html = product_response.text\n",
    "    product_soup = BeautifulSoup(product_html, 'html.parser')\n",
    "\n",
    "    # --------------------- number of reviews ---------------------\n",
    "\n",
    "    # get the number of reviews. e.g. for 256 we iterate 26 times, for 5 we iterate 1 time\n",
    "    # res = product_soup.find_all('p', class_='small semibold font-size-sm text-muted')\n",
    "    # reviews_number = re.search(r'\\d+', str(res[0])).group()\n",
    "\n",
    "    # ^^^^^ this approach got dumped because it was only working for products \n",
    "    #       that actually had any number of reviews\n",
    "\n",
    "    offset = 0\n",
    "\n",
    "    params = {\n",
    "        \"source_id\": 7,\n",
    "        \"page[limit]\": 10,\n",
    "        \"page[offset]\": offset,\n",
    "        \"sort[created]\": \"desc\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(endpoint_url, params=params)\n",
    "    data = response.json()\n",
    "    # print(data)\n",
    "    reviews_number = data['reviews']['count']\n",
    "    reviews_number = int(reviews_number)\n",
    "\n",
    "    if reviews_number <= 0:\n",
    "        return []\n",
    "\n",
    "    # --------------------- product title ---------------------\n",
    "\n",
    "    product_title = product_soup.find('h1', class_='page-title').get_text()\n",
    "    # get rid of multiple whitespaces and \\n\n",
    "    product_title = re.sub(r'\\s+', ' ', product_title).strip()\n",
    "\n",
    "    # --------------------- product price ---------------------\n",
    "\n",
    "    # product_price = product_soup.find('p', class_='product-new-price').get_text()\n",
    "    # # get rid of ' Lei'\n",
    "    # product_price = product_price[:-4] \n",
    "    # # transform string '1.920,00' to float '1920.0'\n",
    "    # product_price = float(product_price.replace('.', '').replace(',', '.')) \n",
    "\n",
    "    # ^^^^^ this approach got dumped\n",
    "\n",
    "    # used this approach instead of looking through the HTML tags (like above)\n",
    "    # because formats were too many, e.g. 'de la xxx.xx Lei' when multiple offers exist\n",
    "\n",
    "    scripts = product_soup.find_all('script')\n",
    "    for script in scripts:\n",
    "        if 'EM.product_id' in script.text:\n",
    "            product_id_search = re.search(r'EM.productDiscountedPrice\\s=\\s([0-9.]+);', script.text)\n",
    "            if product_id_search:\n",
    "                product_price = float(product_id_search.group(1))\n",
    "                break\n",
    "\n",
    "    # --------------------- get reviews ---------------------\n",
    "\n",
    "    review_titles_arr = []\n",
    "    review_ratings_arr = []\n",
    "    review_contents_arr = []\n",
    "    review_verified_users_arr = []\n",
    "\n",
    "    while offset < reviews_number and offset < 50:\n",
    "        params = {\n",
    "            \"source_id\": 7,\n",
    "            \"page[limit]\": 10,\n",
    "            \"page[offset]\": offset,\n",
    "            \"sort[created]\": \"desc\"\n",
    "        }\n",
    "\n",
    "        response = requests.get(endpoint_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        items = data['reviews']['items']\n",
    "\n",
    "        review_titles = [item['title'] for item in items]\n",
    "        review_ratings = [item['rating'] for item in items]\n",
    "        review_contents = [item['content'] for item in items]\n",
    "        review_verified_users = [item['is_bought'] for item in items]\n",
    "\n",
    "        review_titles_arr += review_titles\n",
    "        review_ratings_arr += review_ratings\n",
    "        review_contents_arr += review_contents\n",
    "        review_verified_users_arr += review_verified_users\n",
    "\n",
    "        offset += 10\n",
    "\n",
    "    # --------------------- final product array ---------------------\n",
    "\n",
    "    merged_list = [\n",
    "        {\n",
    "            'product_title': product_title,\n",
    "            'product_price': product_price,\n",
    "            'review_title': review_title, \n",
    "            'review_rating': review_rating, \n",
    "            'review_verified_buyer': review_verified_buyer, \n",
    "            'review_content': review_content\n",
    "        }\n",
    "        for review_title, review_content, review_rating, review_verified_buyer in zip(review_titles_arr, review_contents_arr, review_ratings_arr, review_verified_users_arr)\n",
    "    ]\n",
    "\n",
    "    return merged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- get current listings' indices ---------------------\n",
    "\n",
    "def get_current_page_listings_indices(url):\n",
    "    page_response = requests.get(url)\n",
    "    page_html = page_response.text\n",
    "    page_soup = BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        listings = page_soup.find('div', class_='control-label small hidden-xs hidden-sm js-listing-pagination').get_text()\n",
    "    except AttributeError:\n",
    "        return [-1, -1]\n",
    "\n",
    "    if listings is not None:\n",
    "        # Define the pattern to match the numbers, like:\n",
    "        # '1 - 5 din 5 produse'\n",
    "        # '1 - 60 din 262 de produse'\n",
    "        pattern = r'(\\d+)\\D+(\\d+)\\D+(\\d+)'\n",
    "\n",
    "        # Extract the numbers from input1\n",
    "        match1 = re.search(pattern, listings)\n",
    "        k1 = int(match1.group(1))\n",
    "        k2 = int(match1.group(2))\n",
    "\n",
    "        k3 = int (match1.group(3))\n",
    "\n",
    "        return [k1, k2]\n",
    "    \n",
    "\n",
    "# --------------------- get the total number of listings for the current search ---------------------    \n",
    "\n",
    "def get_total_number_of_listings(url):\n",
    "    page_response = requests.get(url)\n",
    "    page_html = page_response.text\n",
    "    page_soup = BeautifulSoup(page_html, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        listings = page_soup.find('div', class_='control-label small hidden-xs hidden-sm js-listing-pagination').get_text()\n",
    "    except AttributeError:\n",
    "        return -1\n",
    "\n",
    "    if listings is not None:\n",
    "        # Define the pattern to match the numbers, like:\n",
    "        # '1 - 5 din 5 produse'\n",
    "        # '1 - 60 din 262 de produse'\n",
    "        pattern = r'(\\d+)\\D+(\\d+)\\D+(\\d+)'\n",
    "\n",
    "        # Extract the numbers from input1\n",
    "        match1 = re.search(pattern, listings)\n",
    "        k = int(match1.group(3))\n",
    "\n",
    "        return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- get current listings' URLs ---------------------\n",
    "\n",
    "def get_current_page_product_links(url):\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    product_links = soup.find_all('a', class_='card-v2-title semibold mrg-btm-xxs js-product-url')    \n",
    "\n",
    "    return product_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# --------------------- main function for getting all reviews for all search results ---------------------\n",
    "\n",
    "def get_total_reviews(url):\n",
    "    total_reviews = []\n",
    "    total_products = get_total_number_of_listings(url)\n",
    "    print(f\"total products: {total_products}\")\n",
    "\n",
    "    if total_products > 0:\n",
    "        rounds = math.ceil(float(total_products / 60))\n",
    "        print(f\"rounds: {rounds}\")\n",
    "\n",
    "        k = 0\n",
    "        page_number = 1\n",
    "        final_url = url\n",
    "\n",
    "        [from_product, to_product] = get_current_page_listings_indices(url)\n",
    "\n",
    "        for i in range(rounds):\n",
    "            print(f\"from: {from_product}, to: {to_product}, total: {total_products}\")\n",
    "            product_links = get_current_page_product_links(final_url)\n",
    "\n",
    "            # aici e ultima pagina\n",
    "            if i == rounds - 1:\n",
    "                # facem de total % 60 ori\n",
    "                limit = total_products % 60\n",
    "            else:\n",
    "                # facem de 60 de ori\n",
    "                limit = 60\n",
    "\n",
    "            for j, item in enumerate(product_links):\n",
    "                if j >= limit:\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"getting reviews for {item['href']}\")\n",
    "                    total_reviews += get_reviews_for_product_by_url(item['href'])\n",
    "\n",
    "            page_number += 1\n",
    "            \n",
    "            final_url = f'{url}p{page_number}'\n",
    "            [from_product, to_product] = get_current_page_listings_indices(final_url)\n",
    "        \n",
    "    return total_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.emag.ro/search/monitor dell/' # 140 rez\n",
    "# url = 'https://www.emag.ro/search/monitor benq/' # 81 rez\n",
    "# url = 'https://www.emag.ro/search/sony%20wh-1000xm5/' # 3 rez\n",
    "# url = 'https://www.emag.ro/search/hrana-pentru-pisici/pret,intre-105-si-191/Hrana+uscata+pentru+pisici+Acana+Wild+Prairie+NEW%2C+4.5+kg/'\n",
    "url = 'https://www.emag.ro/search/iphone 14 pro max/' # 108 rez\n",
    "# url = 'https://www.emag.ro/search/macbook pro stand lemn maro negru/'\n",
    "    # MUST HAVE / AT END!!!!!\n",
    "\n",
    "reviews = get_total_reviews(url)\n",
    "\n",
    "if reviews is not None:\n",
    "    for review in reviews:\n",
    "        print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- write all reviews to a CSV file ---------------------\n",
    "\n",
    "import csv\n",
    "def write_reviews_to_csv(reviews_param, file_name):\n",
    "    file_csv = file_name\n",
    "\n",
    "    header = reviews_param[0].keys()\n",
    "\n",
    "    with open(file_csv, 'w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(reviews_param)\n",
    "\n",
    "    print(f\"Data has been written to {file_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_reviews_to_csv(reviews, 'reviews_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file = 'reviews_output.csv'\n",
    "\n",
    "# Define the column names and their respective data types\n",
    "column_names = ['product_title', 'product_price', 'review_title', 'review_rating', 'review_verified_buyer', 'review_content']\n",
    "column_types = {'product_title': str, 'product_price': float, 'review_title': str, 'review_rating': int, 'review_verified_buyer': bool, 'review_content': str}\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "reviews_df = pd.read_csv(csv_file, names=column_names, dtype=column_types, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install demoji\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install uuid\n",
    "%pip install json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, uuid, json\n",
    "\n",
    "with open('connectivity_secrets.json') as file:\n",
    "    data = json.load(file)\n",
    "azure_translator_key = data['azure_translator_key']\n",
    "\n",
    "def translate_text(text):\n",
    "    # Add your key and endpoint\n",
    "    key = azure_translator_key\n",
    "    endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
    "\n",
    "    # location, also known as region.\n",
    "    # required if you're using a multi-service or regional (not global) resource. It can be found in the Azure portal on the Keys and Endpoint page.\n",
    "    location = \"francecentral\"\n",
    "\n",
    "    path = '/translate'\n",
    "    constructed_url = endpoint + path\n",
    "\n",
    "    params = {\n",
    "        'api-version': '3.0',\n",
    "        'from': 'ro',\n",
    "        'to': ['en']\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        'Ocp-Apim-Subscription-Key': key,\n",
    "        # location required if you're using a multi-service or regional (not global) resource.\n",
    "        'Ocp-Apim-Subscription-Region': location,\n",
    "        'Content-type': 'application/json',\n",
    "        'X-ClientTraceId': str(uuid.uuid4())\n",
    "    }\n",
    "\n",
    "    # You can pass more than one object in body.\n",
    "    body = [{\n",
    "        'text': text\n",
    "    }]\n",
    "\n",
    "    request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
    "    response = request.json()\n",
    "\n",
    "    # print(json.dumps(response, sort_keys=True, ensure_ascii=False, indent=4, separators=(',', ': ')))\n",
    "\n",
    "    return response[0]['translations'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop reviews that exceed 2500 chars as they would be too resource consuming for our character-limited translation API\n",
    "reviews_df = reviews_df[reviews_df['review_content'].str.len() <= 2500]\n",
    "\n",
    "# drop where we find NaN for title or content\n",
    "reviews_df = reviews_df.dropna(subset=['review_title', 'review_content'])\n",
    "\n",
    "# drop duplicate reviews (e.g., A 'deep purple' iPhone will have the exact same reviews as a 'emerald green' one)\n",
    "reviews_df.drop_duplicates(subset=['product_title', 'product_price', 'review_title', 'review_rating', 'review_verified_buyer', 'review_content'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate the 'title' and 'content' columns from RO -> EN\n",
    "reviews_df['review_title_en'] = reviews_df['review_title'].apply(translate_text)\n",
    "reviews_df['review_content_en'] = reviews_df['review_content'].apply(translate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.to_csv('reviews_translated_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews_df = pd.read_csv('reviews_translated_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.drop(['review_title', 'review_content'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- 2. Second Topic: Data Cleaning & Preprocessing ---------------------\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def preprocess(text):\n",
    "    # remove html tags\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    text = text.replace('<br />', '')\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize words\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and lemmatize words\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalnum() and word.lower() not in stop_words]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df['review_content_en'] = reviews_df['review_content_en'].astype(str)\n",
    "reviews_df['review_title_en'] = reviews_df['review_title_en'].astype(str)\n",
    "\n",
    "# count how many words we have BEFORE preprocessing the data\n",
    "reviews_df['word_count_before'] = reviews_df['review_content_en'].str.split().apply(len) + reviews_df['review_title_en'].str.split().apply(len)\n",
    "\n",
    "# remove special characters and digits from 'review_title_en' and 'review_content_en'\n",
    "reviews_df['review_title_en'] = reviews_df['review_title_en'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "reviews_df['review_content_en'] = reviews_df['review_content_en'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "reviews_df['review_content_en'] = reviews_df['review_content_en'].apply(preprocess)\n",
    "\n",
    "# count how many words we have AFTER preprocessing the data\n",
    "reviews_df['word_count_after'] = reviews_df['review_content_en'].str.split().apply(len) + reviews_df['review_title_en'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "trace_before = go.Scatter(\n",
    "    x=reviews_df.index,\n",
    "    y=reviews_df['word_count_before'],\n",
    "    mode='lines',\n",
    "    name='Words Before Preprocessing'\n",
    ")\n",
    "\n",
    "trace_removed = go.Scatter(\n",
    "    x=reviews_df.index,\n",
    "    y=reviews_df['word_count_before'] - reviews_df['word_count_after'],\n",
    "    mode='lines',\n",
    "    name='Words Removed During Preprocessing'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Number of Words Before and After Preprocessing',\n",
    "    xaxis=dict(title='Index'),\n",
    "    yaxis=dict(title='Number of Words'),\n",
    "    showlegend=True,\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace_before, trace_removed], layout=layout)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- 3 and 4. Third and Fourth Topics: TF-IDF, Feature Importance Analysis ---------------------\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = reviews_df.copy()\n",
    "\n",
    "df['review_verified_buyer'] = df['review_verified_buyer'].astype(int)\n",
    "\n",
    "vectorizer1 = TfidfVectorizer()\n",
    "vectorizer2 = TfidfVectorizer()\n",
    "vectorizer3 = TfidfVectorizer()\n",
    "\n",
    "# fit and transform 'review_title_en', 'review_content_en' and 'product_title' columns\n",
    "tfidf_review_title = vectorizer1.fit_transform(df['review_title_en'])\n",
    "tfidf_review_content = vectorizer2.fit_transform(df['review_content_en'])\n",
    "tfidf_product_title = vectorizer3.fit_transform(df['product_title'])\n",
    "\n",
    "# stack all numeric and tf-idf features together\n",
    "X = hstack([df[['product_price', 'review_verified_buyer']], tfidf_review_title, tfidf_review_content, tfidf_product_title])\n",
    "\n",
    "# target\n",
    "y = df['review_rating']\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "# feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "def plot_top_features(vectorizer, importances, top_n, plot_title):\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    tfidf_importances = importances[:len(feature_names)]\n",
    "\n",
    "    features_importances = list(zip(feature_names, tfidf_importances))\n",
    "\n",
    "    sorted_features_importances = sorted(features_importances, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # top n features\n",
    "    top_features = sorted_features_importances[:top_n]\n",
    "\n",
    "    words, importances = zip(*top_features)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.barh(words, importances, color='skyblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Top {} Features for {}'.format(top_n, plot_title))\n",
    "    plt.gca().invert_yaxis()  # Invert y axis to show the top feature at the top\n",
    "    plt.show()\n",
    "\n",
    "top_n = 10\n",
    "\n",
    "start1 = 2\n",
    "end1 = 2 + tfidf_review_title.shape[1]\n",
    "start2 = end1\n",
    "end2 = start2 + tfidf_review_content.shape[1]\n",
    "start3 = end2\n",
    "end3 = start3 + tfidf_product_title.shape[1]\n",
    "\n",
    "plot_top_features(vectorizer1, importances[start1:end1], top_n, 'review_title_en')\n",
    "plot_top_features(vectorizer2, importances[start2:end2], top_n, 'review_content_en')\n",
    "plot_top_features(vectorizer3, importances[start3:end3], top_n, 'product_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- 5. Fifth Topic: Topic Modelling ---------------------\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "reviews = reviews_df['review_content_en'].str.split()\n",
    "\n",
    "dictionary = corpora.Dictionary(reviews)\n",
    "\n",
    "corpus = [dictionary.doc2bow(review) for review in reviews]\n",
    "\n",
    "# train the LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "topics = ldamodel.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "vis = gensimvis.prepare(ldamodel, corpus, dictionary, n_jobs=1)\n",
    "\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- 6. Sixth Topic: Sentiment Analysis ---------------------\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "reviews_df['sentiment_score'] = reviews_df['review_content_en'].apply(lambda x: sia.polarity_scores(str(x))['compound'])\n",
    "\n",
    "reviews_df['sentiment_label'] = reviews_df['sentiment_score'].apply(lambda x: 'Positive' if x > 0.05 else ('Negative' if x < -0.05 else 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentiments = reviews_df['sentiment_label'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.pie(sentiments, labels=sentiments.index, startangle=90, autopct='%1.1f%%',\n",
    "        colors=['#98FB98', '#F08080', '#A9A9A9'])  # lightgreen, lightcoral, darkgray\n",
    "\n",
    "centre_circle = plt.Circle((0,0),0.70,fc='white')\n",
    "fig = plt.gcf()\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "plt.axis('equal')  \n",
    "\n",
    "plt.title('Sentiment Analysis Distribution', size=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm_project",
   "language": "python",
   "name": "dm_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
